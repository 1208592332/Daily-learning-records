## 基于机器学习的web异常检测
<font size=3 face="黑体">&emsp;&emsp;传统web传统web入侵检测技术通过维护规则集对入侵访问进行拦截。一方面，硬规则在灵活的黑客面前，很容易被绕过，且基于以往知识的规则集难以应对0day攻击；另一方面，攻防对抗水涨船高，防守方规则的构造和维护门槛高、成本大。入侵检测技术通过维护规则集对入侵访问进行拦截。
&emsp;&emsp;**基于机器学习技术的新一代web入侵检测技术有望弥补传统规则集方法的不足，为web对抗的防守端带来新的发展和突破。**机器学习方法能够基于大量数据进行自动化学习和训练，已经在图像、语音、自然语言处理等方面广泛应用。然而，机器学习应用于web入侵检测也存在挑战，其中最大的困难就是标签数据的缺乏。尽管有大量的正常访问流量数据，但web入侵样本稀少，且变化多样，对模型的学习和训练造成困难。因此，目前大多数web入侵检测都是基于无监督的方法，针对大量正常日志建立模型(Profile)，而与正常流量不符的则被识别为异常。这个思路与拦截规则的构造恰恰相反。拦截规则意在识别入侵行为，因而需要在对抗中“随机应变”；而基于profile的方法旨在建模正常流量，在对抗中“以不变应万变”，且更难被绕过。</font>

![](https://image-static.segmentfault.com/154/316/1543168537-58e866e9262d7_articlex)

<font size=3>基于异常检测的web入侵识别，训练阶段通常需要针对每个url，基于大量正常样本，抽象出能够描述样本集的统计学或机器学习模型(Profile)。检测阶段，通过判断web访问是否与Profile相符，来识别异常。</font>

![](https://image-static.segmentfault.com/253/921/253921652-58e866e925d40_articlex)

**对于Profile的建立，主要有以下几种思路：**
<font size=3>**1. 基于统计学习模型**

基于统计学习的web异常检测，通常需要对正常流量进行数值化的特征提取和分析。特征例如，URL参数个数、参数值长度的均值和方差、参数字符分布、URL的访问频率等等。接着，通过对大量样本进行特征分布统计，建立数学模型，进而通过统计学方法进行异常检测。
**2. 基于文本分析的机器学习模型**

Web异常检测归根结底还是基于日志文本的分析，因而可以借鉴NLP中的一些方法思路，进行文本分析建模。这其中，比较成功的是基于隐马尔科夫模型(HMM)的参数值异常检测。
**3. 基于单分类模型**

由于web入侵黑样本稀少，传统监督学习方法难以训练。基于白样本的异常检测，可以通过非监督或单分类模型进行样本学习，构造能够充分表达白样本的最小模型作为Profile，实现异常检测。
**4. 基于聚类模型**

通常正常流量是大量重复性存在的，而入侵行为则极为稀少。因此，通过web访问的聚类分析，可以识别大量正常行为之外，小搓的异常行为，进行入侵发现。</font>

![](https://image-static.segmentfault.com/300/398/3003988737-58e866e95e992_articlex)


<font color=#00ffff size=4 face="黑体">&emsp;&emsp;hihttps是一款免费的web应用防火墙，既支持传统WAF的所有功能如SQL注入、XSS、恶意漏洞扫描、密码暴力破解、CC、DDOS等，又支持无监督机器学习，自主对抗，重新定义web安全。</font>

<font face="黑体" size=4>&emsp;&emsp;下面是hihttps通过机器学习自动生成对抗规则的5个过程：</font>

![学习过程](https://image.3001.net/images/20200205/1580873974_5e3a38f611db4.png!small)

## 一、样本采集
<font face="黑体" size=4>&emsp;&emsp;和图形图像的人工智能一样，机器学习无论是有监督还是无监督，第一步都是先采集样本。web安全有先天性的样本采集优势，成本几乎为0，方法是：通过反向代理的模式采集完整的HTTP协议数据，参考hihttps源码https://github.com/qq4108863/hihttps/  </font>
1. 足够的随机化，在不同的IP地址之间随机采集。
2. 足够多的样本，保证99.99%的正确率，至少需要采集数万份的样本。
3. 足够的时间，至少在不同的时间段采集3-7天的样本。
4. 尽量是正常流量下采集，减少样本没有被黑客攻击污染的可能性。
5. 完整的数据，样本包括全部的HTTP 请求头和body。

<font face="黑体" size=4>&emsp;&emsp;基于机器学习的web应用防火墙hiihttp是怎样工作的呢？比如有个网站接口，在正常情况是通过http://www.hihttps.com/hihttps?id=xxx&token=xxx 的形式访问，hihttps先把采集到样本如URL参数，保存在train训练目录下，样本文件主要内容如下：</font>

```
        GET /hihttps?id=123&token=2458-a632-3d56-a9bf

        GET /hihttps?id=238&token=ce58-a49d-b767-68ed

        GET /hihttps?id=523&token=2bd8-c4d2-d324-29b3

        GET /hihttps?id=abc&token=2bd8-c4d2-d324-29b3

        GET /hihttps?id=abc%20’or 1=1’

        ……
```
<font face="黑体" size=4>&emsp;&emsp;当采集到的样本数量，达到一定数量（如1万），hihttps机器学习引擎就开始启动，第一步就是滤噪。 </font>

## 二、滤噪
<font face="黑体" size=4>&emsp;&emsp;在正常的情况下，拿到的样本绝大多数是大量重复性存在的，但是也不排除样本存在黑客攻击，也就是说，个别样本可能已经被污染了，hihttps在降维和特征提取之前先过滤。
&emsp;&emsp;滤噪的方法通常是用聚类方法，把样本分为两类，把其中小于3%的样本去掉，通常有以下几种做法：</font>
 <font face="黑体" size=4>&emsp;&emsp;1：URL参数过滤。比如正常情况下是/hihttps?id=xxx&token=xxx，那么如果有小于1%的/hihttps?sql=xxx，那么就要过滤这条样本。
&emsp;&emsp;2：URL长度过滤。一般来说，URL长度值分布，均值μ，方差σ3，在切比雪夫不等式范围外，要过滤掉。
&emsp;&emsp;3：参数值长度过滤。一般来说，参数如tolken=xxx，其中xxx的长度值分布，均值μ，方差σ2，在切比雪夫不等式范围外，要过滤掉。
&emsp;&emsp;4：SQL注入过滤。用libinjection库查一遍，符合SQL注入特征的样本要过滤。
&emsp;&emsp;5：XSS攻击过滤。用libinjection库查一遍，符合XSS特征的样本要过滤。
&emsp;&emsp;6：其他已知攻击过滤。如ModSecurity 的OWASP规则很牛，先跑一遍过滤。
经过滤噪处理后，我们把样本就分为正常和异常样本，正常的如下：
</font>
```
        GET /hihttps?id=123&token=2458-a632-3d56-a9bf

        GET /hihttps?id=238&token=ce58-a49d-b767-68ed

        GET /hihttps?id=523&token=2bd8-c4d2-d324-29b3

        GET /hihttps?id=abc&token=2bd8-c4d2-d324-29b3

         …
```
<font face="黑体" size=4>少数异常样本，如疑似SQL注入攻击则去掉</font>
```
   GET /hihttps?id=abc%20’or 1=1’
   ……
```

<font face="黑体" size=4>整个过程，无监督进行，可以用到的数学算法有**K均值（K-Mean）、主成分分析PCA、切比雪夫不等式、高斯混合模型GMM、稀疏矩阵……**</font>

## 三、降维
<font face="黑体" size=4>&emsp;&emsp;滤噪后最重要的一步就是降维，这是机器学习的核心。降维就是通过特定的数学算法，把复杂的东西，用特征表达向量，变为机器可以理解的东东，降维方法分为线性降维（PCA 、ICA LDA、LFA、LPP等）和非线性降维KPCA 、KICA、KDA、ISOMAP、LLE、LE、LPP、LTSA、MVU等）。
&emsp;&emsp;怎么让机器理解/hihttps?id=abc%20’or 1=1’这就是一条攻击呢？在web安全领域和图形图像完全不同，主要就是涉及自然语言处理，尤其是文本的识别，主要有下面几种模型：</font>
#### 1.词袋模型
  <font face="黑体" size=4>&emsp;&emsp;文本的降维本质上涉及到了文本的表达形式。在传统的词袋模型当中，对于每一个词采用one-hot稀疏编码的形式，假设目标语料中共有N个唯一确认的词，那么需要一个长度N的词典，词典的每一个位置表达了文本中出现的某一个词。在某一种特征表达下，比如词频、binary、tf-idf等，可以将任意词，或者文本表达在一个N维的向量空间里。凭借该向量空间的表达，可以使用机器学习算法，进行后续任务处理。
&emsp;&emsp;这种方式被称为**n-gram语法**，指文本中连续出现的n个语词。当n分别为1、2、3时，又分别称为**一元语法（unigram）、二元语法（bigram）与三元语法（trigram）**。 </font>
#### 2.维度选择
<font face="黑体" size=4>&emsp;&emsp;常用的有卡方、互信息这种统计检验的方法；还有借助机器学习模型降维的方法。比如，使用随机森林，或者逻辑回归等模型，筛选出那些在分类任务中具有较大特征重要性，或者系数绝对值较大的TOP特征作为降维后的特征**。 </font>
#### 3.主题模型
<font face="黑体" size=4>&emsp;&emsp;主题模型同时具备了降维和语义表达的效果，比如LSI、LDA、PLSA、HDP等统计主题模型，这些模型寻求文本在低维空间（不同主题上）的表达，在降低维度的同时，尽可能保留原有文本的语义信息。
 </font>
#### 4.神经网络
 <font face="黑体" size=4>&emsp;&emsp;如卷积神经CNN、循环神经RNN等。
&emsp;&emsp;理论可能有点复杂，那我们直接拿4条样本来举例说明吧：</font>
```
        GET /hihttps?id=123&token=2458-a632-3d56-a9bf

        GET /hihttps?id=238&token=ce58-a49d-b767-68ed

        GET /hihttps?id=523&token=2bd8-c4d2-d324-29b3

        GET /hihttps?id=abc&token=2bd8-c4d2-d324-29b3

         …..
```
<font face="黑体" size=4>&emsp;&emsp;降维的目的就是为了让机器能够理解id是什么，token又是什么，什么情况是攻击。我们先来定义一些稀疏编码：</font>
```
        N：整数，0-9

        C：字符，a-z

        X: 16进制数字，0-9 a-f

        D:标点分隔符.-|

        ……..
```
<font face="黑体" size=4>&emsp;&emsp;GET /hihttps?id=123&token=2458-a632-3d56-a9bf 这种我们就用稀疏编码把其维度降为id=N&token=XDXDXDX，这样机器就可能理解了，哦，原来id就是数字嘛。
&emsp;&emsp; 当然这是最简单的情况，实际场景可能很复杂，比如10.1究竟是代表数字？或者钱？或者版本号呢？就需要做更多的参数关联运算（如money=xx或者version=xx）。如果我们观察到的样本，大于99.9%的参数id=都是数字，就可以认为GET /hihttps?id=abc就是一条非法攻击，这就是机器学习能够检测未知攻击的核心原理。
&emsp;&emsp; 实际生产环境中情况更复杂的，所以让机器达到网络专家的智能水平，还有很长的路要走，但这是必然的发展方向。</font>
## 四、特征选择

<font face="黑体" size=4>&emsp;&emsp;下一步，hihttps就是对正常流量进行数值化的特征提取和分析。通过对大量样本进行特征分布统计，建立数学模型，特征提取包括：URL参数个数、参数值长度的均值和方差、参数字符分布、URL的访问频率等等。如下表所示：</font>


<table>
	<tr>
	    <th>类别</th>
	    <th>序号</th>
	    <th>特征名称</th>
	    <th>特征描述</th>
	</tr >
	<tr >
	    <td rowspan="20">语法特征</td>
	    <td>1</td>
	    <td>Topic_len</td>
	    <td>TOPIC 长度</td>
	</tr>
	<tr>
	    <td>2</td>
	    <td>Path_len</td>
	    <td>路径长度</td>
	</tr>
	<tr>
	    <td>3</td>
	    <td>Path</td>
	    <td>路径最大长度</td>
	</tr>
	<tr>
	    <td>4</td>
	    <td>Path_Maxlen</td>
	    <td>路径平均长度</td>
	</tr>
	<tr>
	    <td>5</td>
	    <td>Argument_len</td>
	    <td>参数部分长度</td>
	</tr>
	<tr>
	    <td>6</td>
	    <td>Name_Max_len</td>
	    <td>参数名最大长度</td>
	</tr>
	<tr>
	    <td>7</td>
	    <td>Name_Avglen</td>
	    <td>参数名平均长度</td>
	</tr>
	<tr>
	    <td>8</td>
	    <td>Value_Max_len</td>
	    <td>参数值最大长度</td>
	</tr>
	<tr>
	    <td>9</td>
	    <td>Value_Avg_len</td>
	    <td>参数值平均长度</td>
	</tr>
	<tr>
	    <td>10</td>
	    <td>Argument_len</td>
	    <td>参数个数</td>
	</tr>
	<tr>
	    <td>11</td>
	    <td>String_Max_len</td>
	    <td>字符串最大长度</td>
	</tr>
	<tr>
	    <td>12</td>
	    <td>Number_Maxlen</td>
	    <td>连续数字最大长度</td>
	</tr>
	<tr>
	    <td>13</td>
	    <td>Path_number</td>
	    <td>路径中的数字个数</td>
	</tr>
	<tr>
	    <td>14</td>
	    <td>Unknow_len</td>
	    <td>特殊字符的个数</td>
	</tr>
	<tr>
	    <td>15</td>
	    <td>Number_Percentage</td>
	    <td>参数值中数字占有比例</td>
	</tr>
	<tr>
	    <td>16</td>
	    <td>String_Percentage</td>
	    <td>参数值字母占有比例</td>
	<tr>
	<tr>
	    <td>17</td>
	    <td>Unkown_Percentage</td>
	    <td>参数值中特殊字符的比例</td>
	</tr>
	<tr>
	    <td>18</td>
	    <td>BigString_Percentage</td>
	    <td>大写字符所占比例</td>
	</tr>
	<tr>
	    <td>19</td>
	    <td>Spacing_Precentage</td>
	    <td>空格字符所占比例</td>
	</tr>
	<tr >
	    <td rowspan="5">攻击特征</td>
	    <td>20</td>
	    <td>ContainIP</td>
	    <td>参数值是否包含IP</td>
	</tr>
	<tr>
	    <td>21</td>
	    <td >Sql_Risk_level</td>
	    <td>SQL 类型危险等级</td>
	</tr>
	<tr>
	    <td>22</td>
	    <td >Xss_Risk_level</td>
	    <td>Xss 类型危险等级</td>
	</tr>
	<tr>
	    <td>23</td>
	    <td >Others_Risk_level</td>
	    <td>其他类型危险等级</td>
	</tr>
	<tr >
	    <td rowspan="1">自然语言</td>
	    <td>24</td>
	    <td>NLP</td>
	    <td>自然语言理解处理</td>
	</tr>
</table>

## 五、生成对抗规则
<font face="黑体" size=4>&emsp;&emsp;最后hihttps通过大量的样本采集，精确给这个/hihttps?id=xxx&token=xxx接口参数，生成对抗规则，保存在rule目录下，和传统WAF的ModSecurity的OWASP规则放在一起，保护网站不被攻击。
&emsp;&emsp;下面的一律视为攻击，只有机器学习才有可能检测未知攻击，这是网络安全专家也难以做到的。</font>
```
        GET /hihttps?id=123    参数缺失
        GET /hihttps?id=abc&token=2458-a632-3d56-a9bf   id参数不对
        GET /hihttps?admin=%0acdef         未知攻击
        ….....
```
<font face="黑体" size=4>&emsp;&emsp;最后总结如下:</font>
```
        1、整个过程完全是无监督的机器学习，有些特殊的参数，也可以由网络安全专家人为干预半监督，从而从99.9%到100%准确率的进化。

        2、传统的waf规则很难对付未知漏洞和未知攻击。让机器像人一样学习，具有一定智能自动对抗APT攻击或许是唯一有效途径，但黑客技术本身就是人类最顶尖智力的较量，WEB安全仍然任重而道远。

        3、幸好hihttps这类免费的应用防火墙在机器学习、自主对抗中开了很好一个头，未来web安全很可能是特征工程+机器学习共同完成，未来WEB安全必然是AI的天下。
```



### 聚类算法

### 1.聚类方法定义
<font face="黑体" size=4>&emsp;&emsp;聚类方法是针对给定样本，依据它们的特征的相似度或距离，将其归并到若干个“类”或“簇”的数据分析问题，一个类是给定样本集合的一个子集。
&emsp;&emsp;聚类的基本目的时通过得到的类或簇来发现数据的特点或对数据进行处理。属于无监督学习，因为只是根据样本的相似度或距离将其归类，而类或簇是事先并不知道的。
&emsp;&emsp;最常用的聚类算法：层次聚类（hierarchical clustering）和K均值聚类（k-means clustering）。层次聚类又有聚合（自下而上）和分裂（自下而上）两种方法。聚合法开始讲每个样本各自分到一个类；之后将相聚最近的另类合并，建立新类，重复直到满足停止条件；得到层次化的类别。分裂法开始将所有样本分到一个类；之后将已有类中相距最远的样本分到两个新的类中，重复此操作直到满足停止条件，得到层次化类别。K均值聚类是基于中心的聚类算法，通过迭代，将样本分到k个类中，使得每个样本与其所属类中的中心或均值最近，得到k个“平坦的”、分层次化的类别，构成对空间的划分。</font>
### 2.聚类的基本概念
#### 2.1 相似度距离
<font face="黑体" size=4>&emsp;&emsp;聚类的核心是相似度（similarrity）或距离（distance）有多种相似度或距离的定义。相似度直接影响聚类的结果，所以其选择是聚类的根本问题。具体哪种相似度更适合取决于应用问题的特性。</font>
##### 2.1.1闵可夫斯基距离
<font face="黑体" size=4>&emsp;&emsp;聚类中,将样本集合看做向量空间中的点的集合，以空间距离表示样本之间的相似度。常用的距离闵可夫斯基距离，特别的欧氏距离。闵可夫斯基距离越大相似度越小，距离越小相似度越大。</font>
<font face="黑体" size=4>&emsp;&emsp;**定义:**
&emsp;&emsp;定义样本集合X，X是m维实数向量空间$$**R**^{m}$$中点的集合，其中$$x_i$$,$$x_j$$ $$\in{X}$$, $$x_i$$=$$(x_{1i},x_{2i},\cdots x_{mi} )^T$$, $$x_j$$=$$(x_{1j},x_{2j},\cdots x_{mj} )^T$$,样本$$x_i$$,$$x_j$$的闵可夫斯基距离定义为：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$d_{ij}$$=$$(\sum_{k=1}^{m}|x_{ki}-x_{kj}|^{p})^{1/p}$$
这里$$p\geq1$$。当p=2时称为欧氏距离，即：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$d_{ij}$$=$$(\sum_{k=1}^{m}|x_{ki}-x_{kj}|^{2})^{1/2}$$
当p=1时成为曼哈顿距离，即：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$d_{ij}$$=$$\sum_{k=1}^{m}|x_{ki}-x_{kj}|$$
当$$p=\infty$$时，称为切比雪夫距离，取各个坐标数据差的绝对值的最大值，即：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$d_{ij}$$=$$\max_{k}|x_{ki}-x_{kj}|$$
</font>
##### 2.1.2马哈拉诺比斯距离
<font face="黑体" size=4>&emsp;&emsp;马哈拉诺比斯距离，简称马氏距离，也是一种常用相似度，考虑各个分量（特征）之间的相关性与各个分量的尺度无关。马氏距离越大相似度越小，距离越小相似度越大。
&emsp;&emsp;**定义:**
&emsp;&emsp;给定一个样本集合X，X=$$[x_{ij}]_{m \times n}$$,其协方差矩阵记作S。样本$$x_i$$与$$x_{j}$$之间马氏距离$$d_{ij}$$定义为:
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$d_{ij}=[(x_i-x_j)^{T} (S^{-1})(x_i-x_j)]^{1/2}$$
其中
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$x_i$$=$$(x_{1i},x_{2i},\cdots x_{mi} )^T$$,

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$x_j$$=$$(x_{1j},x_{2j},\cdots x_{mj} )^T$$,
&emsp;&emsp;当S为单位矩阵时，即样本数据的各个分量互相独立且各个分量的方差为1，即可发现上式马氏距离即为欧式距离，即马氏距离是欧式距离的推广。</font>
##### 2.1.3相关系数
<font face="黑体" size=4>&emsp;&emsp;样本之间相似度也可相关系数来表示。相关系数的绝对值越接近于1，表示样本越相似；越接近于0，表示样本越不相似。
&emsp;&emsp;**定义:**
&emsp;&emsp;样本$$x_i$$与样本$$x_j$$之间的相关系数定义为：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$r_{ij}=\frac{\sum_{k=1}^{m}(x_{ki}-\overline x_i)(x_{kj}-\overline x_j)}{[\sum_{k=1}^m (x_{ki}-\overline x_i)^2\sum_{k=1}^m (x_{kj}-\overline x_j)^2]^{1/2}}$$
其中
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$\overline x_i=\frac{1}{m}\sum_{k=1}^m x_{ki}$$,$$\overline x_j=\frac{1}{m}\sum_{k=1}^m x_{kj}$$</font>
##### 2.1.4夹角余弦
<font face="黑体" size=4>&emsp;&emsp;样本之间的相似度也可以用夹角余弦表示，夹角余弦越接近于1样本越相似；越接近于0，表示样本越不相似。
&emsp;&emsp;**定义:**
&emsp;&emsp;样本$$x_i$$与样本$$x_j$$之间的夹角余弦定义为
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$s_{ij}=\frac{\sum_{k=1}^{m}x_{ki}x_{kj}}{[\sum_{k=1}^{m}x_{ki}^2 \sum_{k=1}^2 x_{kj}^2]^\frac{1}{2}}$$
&emsp;&emsp;由上述定义可看出，用距离度量相似度，距离越小样本越相似；用相关系数，值越大样本越相似。不同的度量方式得到的结果并不一定一直</font>
![距离与相关系数的关系](/uploads/zhishifenxiang/images/m_6fd777660f83ad625db1f3ef112d99cc_r.png "距离与相关系数的关系")
<font face="黑体" size=4>&emsp;&emsp;从图中可看出，如果从距离的角度看，A和B比A和C更相似；但从相关系数的角度看，A和C比A和B更相似。故，在进行聚类时，选择合适的距离或相似度非常重要。</font>

#### 2.2类或簇
<font face="黑体" size=4>&emsp;&emsp;通过聚类得到的类或簇，本质是样本的子集。用G表示累活簇，用$$x_i$$,$$x_j$$表示类中的样本，用$$n_G$$表示G中样本个数，用$$d_{ij}$$表示样本$$x_i$$与$$x_j$$之间的距离。类和簇的定义有几种常见的定义。
&emsp;&emsp;**定义：**
&emsp;&emsp;1.设T为给定的正数，集合G中任意两个样本$$x_i$$ $$x_j$$,有
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$d_{ij}\le T$$,
则称G为一个类或簇
&emsp;&emsp;2.设T为给定的正数，集合G中任意样本$$x_i$$,一定存在G中另一个样本$$x_j$$,使得
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$d_{ij}\le T$$,
则称G为一个类或簇
&emsp;&emsp;3.设T为给定的正数，集合G中任意样本$$x_i$$,一定存在G中另一个样本$$x_j$$,使得
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$\frac{1}{n_G -1}\sum_{x_j\in G}d_{ij}\le T$$,
则称G为一个类或簇
&emsp;&emsp;4.设T和V为给定两个正数，如果集合G中任意两个样本$$x_i$$ $$x_j$$的距离满足:
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$\frac{1}{n_G (n_G -1)}\sum_{x_i \in G} \sum_{x_j\in G}d_{ij}\le T$$,
则称G为一个类或簇
&emsp;&emsp;上面四个定义，第一个最常用，可推出其他三个。
类的特征可通过不同角度刻画，常用特征有下面三种：
&emsp;&emsp;（1）类的均值$$\overline x_G$$,又称为类的中心
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$\overline x_G=\frac{1}{n_G} \sum_{i=1}^{n_G}x_i$$
式中$$n_G$$是类G的样本个数
&emsp;&emsp;(2)类的直径$$D_G$$
&emsp;&emsp;类的直径$$D_G$$是勒种任意两个样本之间的最大距离，即
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$D_G ={ max\atop{x_i,x_j\in G}}d_{ij}$$
&emsp;&emsp;(3)类的样本散布矩阵$$A_G$$与样本协方差矩阵$$S_G$$
&emsp;&emsp;类的样本散布矩阵$$A_G$$为
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$A_G=\sum_{i=1}^{n_G}(x_i-\overline x_G)(x_i-\overline x_G)^T$$
&emsp;&emsp;样本协方差矩阵$$S_G$$为
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$S_G=\frac{1}{m-1}A_G=\frac{1}{m-1}\sum_{i=1}^{n_G}(x_i-\overline x_G)(x_i-\overline x_G)^T$$
其中m为样本的维数（样本属性的个数）
</font>
#### 2.3类与类间距离
<font face="黑体" size=4>&emsp;&emsp;考虑类$$G_p$$与类$$G_q$$之间的距离$$D_(p,q)$$，也称为连接，类与类间距离也有多种定义
&emsp;&emsp;设$$G_p$$包含$$n_p$$个样本，$$G_q$$包含$$n_q$$个样本，分别用$$\overline x_p$$和$$\overline x_q$$表示$$G_p$$包含$$n_p$$的均值，即类的中心
&emsp;&emsp;（1）最短（单距离）或最长距离（完全距离）
&emsp;&emsp;定义$$G_p$$包含$$G_q$$的样本之间的最短距离为两类之间的距离
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$D_{pq}$$=$$min\lbrace{d_{ij} | x_i \in G_p,x_j\in G_q} \rbrace$$
&emsp;&emsp;定义$$G_p$$包含$$G_q$$的样本之间的最长距离为两类之间的距离
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$D_{pq}$$=$$max\lbrace{d_{ij} | x_i \in G_p,x_j\in G_q} \rbrace$$
&emsp;&emsp;（2）中心距离
&emsp;&emsp;定义$$G_p$$包含$$G_q$$的样本中心$$\overline x_p$$和$$\overline x_q$$之间的距离为两类之间的距离
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$D_{pq}=d_{\overline x_p \overline x_q}$$
&emsp;&emsp;（3）平均距离
&emsp;&emsp;定义$$G_p$$包含$$G_q$$的任意两个样本的距离的平均值为两类之间的距离
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$D_{pq}=\frac{1}{n_p n_q}\sum_{x_i \in G_p}\sum_{x_j \in G_q}d_{ij}$$
</font>
### 3.聚类
#### 3.1层次聚类
<font face="黑体" size=4>&emsp;&emsp;层次聚类假设类别之间存在层次结构，将样本聚到层次化的类中</font>

![层次聚类的分类](/uploads/zhishifenxiang/images/m_47d96eab052df3e7e40ee306d90c4d38_r.png)
- 因为每个样本只属于一个类，故层次聚类属于硬聚类
<font face="黑体" size=4>&emsp;&emsp;**聚合聚类**开始将每个样本各自分到一个类；之后将相距最近的两类合并，建立一个新类，重复操作直到满足停止条件；**分裂聚类**开始将所有样本分到一个类，之后将已有类中相距最远的样本分到两个新的类，重复操作直到满足停止条件，得到层次化类别。</font>
<font face="黑体" size=4>&emsp;&emsp;聚合聚类的具体过程：对于给定的样本集合，开始将每个样本分到一个类；然后按照一定规则，如类间距离最小，将满足规则条件的两个类合并；反复进行，每次减少一个类，直到满足停止条件，如所有样本聚为一个类。
&emsp;&emsp;聚合聚类需预先确定三个要素：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;(1)距离或相似度
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;(2)合并规则
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;(3)停止条件

&emsp;&emsp;**算法步骤：**
&emsp;&emsp;&emsp;&emsp;输入：n个样本组成的样本集合及样本之间的距离
&emsp;&emsp;&emsp;&emsp;输出：对样本集合的一个层次化聚类
&emsp;&emsp;&emsp;&emsp;（1）计算n个样本两两之间的欧氏距离{$$d_{ij}$$},记作矩阵D=$$[d_{ij}]_{n\times n}$$
&emsp;&emsp;&emsp;&emsp;（2）构造n个类，每个类中只包含一个样本
&emsp;&emsp;&emsp;&emsp;（3）合并类间距离最小的两个类，其中最短距离为类间距离，构建一个新类
&emsp;&emsp;&emsp;&emsp;（4）计算新类与当前各类的距离。若类的个数为1，终止计算，否则回到（3）
&emsp;&emsp;聚合层次聚类的复杂度是$$\omicron(n^3m)$$,其中m是样本的维数，n是样本个数。
</font>
<font face="黑体" size=4>&emsp;&emsp;**例子**
&emsp;&emsp;给定五个样本的集合，样本之间的欧氏距离you矩阵D表示:

![](/uploads/zhishifenxiang/images/m_121e29bca9e46508f1a8e4947db4fb6d_r.png)
其中$$d_{ij}$$表示第i个样本与第y个样本之间的欧氏距离，使用聚合层次聚类对五个样本进行聚类。
**计算：**（1）首先五个样本构建五个类，$$G_i = {x_i}$$, i=1,2,..., 5,样本之间距离转变为类内距离，即D。
&emsp;&emsp;&emsp;（2）在矩阵D中，$$D_{35}=D_{53}=1$$为最小，所以将G5和G3合并为一类，记$$G_6={x_3,x_5}$$。
&emsp;&emsp;&emsp;（3）计算G6与G1，G2，G4之间最短距离，有 $$D_{61}=2,D_{62}=5,D_{64}=5$$
又注意到其余两类之间距离是$$D_{12}=7,D_{14}=9,D_{24}=4$$
显然，$$D_{61}=2$$最小，所以将G1和G6合并为一个$$G_7=\lbrace x_1,x_3,x_5 \rbrace$$。
&emsp;&emsp;&emsp;（4）计算G7与G2，G4之间的最短距离 $$d_{72}=5,D_{74}=5$$
又注意到$$D_{24}=4$$
显然，其中$$D_{24}=4$$最小，所以将G2与G4合并成新类，记$$D_8=\lbrace x_2,x_4 \rbrace$$
&emsp;&emsp;&emsp;（5）将G7与G8合并成一个新类，记$$G_9=\lbrace x_1,x_2,x_3,x_4,x_5\rbrace$$,将样本聚合为一类满足停止条件，聚类终止。
层次聚类图表如下：

![](/uploads/zhishifenxiang/images/m_146507bf34a38ef29182ae6d401414ca_r.png)

</font>

#### 3.2k均值聚类
<font face="黑体" size=4>&emsp;&emsp;k均值聚类是基于样本集合划分的聚类算法。k均值聚类将样本集合划分k个集合，构成k个类，将n个样本分到k个类中，每个样本到其所属类的中心距离最小。属于硬聚类</font>

<font face="黑体" size=4>**模型**
&emsp;&emsp;给定n个样本的集合$$S=\lbrace x_1,x_2,\cdots,x_n
\rbrace$$,每个样本有一个特征向量表示，特征向量的维数是m。k-means的目标是将n个样本分到k个不同的类中或簇中。这里假设k＜n。k个类G1，G2，...Gk形成对样本集合X的划分，其中$$G_i\bigcap G_j=\phi,\bigcup_{i=1}^{k}G_i=X$$。用C表示划分，一个划分对应一个聚类结果。
**策略**
&emsp;&emsp;k均值聚类归为样本集合X的划分，或者从样本到类的函数的选择问题。k均值聚类的策略时通过损失函数的最小化选取最优的划分或函数C*。
&emsp;&emsp;首先，采用欧氏距离平方作为样本之间的距离$$d_{x_i,x_j}$$
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$d_{x_i,x_j}=\sum_{k=1}^m (x_{ki}-x_{kj})^2=||x_i-x_j||^2$$
&emsp;&emsp;然后，定义样本与其所属类的中心之间的距离的总和为损失函数，即：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$W(C)=\sum_{l=1}^k \sum_{C(i)=l} ||x_i-\overline x_j||^2$$
式中$$\overline x_l =(\overline x_{1l},\overline x_{2l},\cdots,\overline x_{ml})^T$$是第l个类的均值或中心，$$n_l=\sum_{i=1}^nI(C(i)=l)$$,其中$$I(C(i)=l)$$是指示函数，取值为1或0.函数W（C）也成为能量，表示想同类中样本相似程度。
&emsp;&emsp;k均值聚类就是求解最优化问题：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$C^\ast=arg min_C W(C)=arg min_C\sum_{k=1}^k \sum_{C(i)=l}||x_i- \overline x_l||^2$$
&emsp;&emsp;相似的样本聚到同类时，损失函数最小，目标函数的最优化能达到聚类的目的。但这是个组合优化问题，n个样本分到k类所有可能的分法：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$S(n,k)=\frac{1}{k!}\sum_{l=1}^k (-1)^{k-l}{k \choose l }k^n$$
这个数字是指数级，k均值聚类的最优解求解问题时NP困难问题，现实采用迭代方法求解。</font>

<font face="黑体" size=4>**算法**
&emsp;&emsp;k均值聚类的算法是一个迭代过程，每次迭代包括两个步骤。首先选择k各类的中心（随机），将样本逐个归到与其最近的中心的类中，得到一个聚类结果；后更新每个类的均值，作为新的中心，重复计算，直至收敛为止。
&emsp;&emsp;首先选取一定的中心值$$(m_1,m_2,\cdots m_k)$$,求一个划分C，使目标函数极小化：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$min_C \sum_{l=1}^k \sum_{C(i)=l}||x_i-\overline x_l||^2$$
即样本与所属中心样本之间的距离总和最小，将样本指派到与其最近的中心$$m_l$$的类中Gl。
&emsp;&emsp;再对给定的划分C，求各个类的中心使得目标函数极小化：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$min_{m_1,\cdots,m_k} \sum_{l=1}^k \sum_{C(i)=l}||x_i-\overline x_l||^2$$
即在划分确定的情况下，是样本与其所属类的中心之间距离总和最小。对每个包含$$n_l$$个样本的类$$G_l$$，更新其均值$$m_l$$:
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$m_l=\frac{1}{n_l}\sum_{C(i)=l} x_i$$,l=1,...k
&emsp;&emsp;重复步骤，知道划分不再改变，聚类停止。

具体过程：
&emsp;&emsp;输入：n个样本的集合X
&emsp;&emsp;输出：样本集合的聚类C*
&emsp;&emsp;(1)初始化。另t=0，随机选择k个样本作为初始聚类中心:
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$m^{(0)}=(m_1^{(0)},\cdots,m_l^{(0)},\cdots,m_k^{(0)})$$
&emsp;&emsp;(2)对样本进行聚类。对固定的类中心$$m^{(t)}=(m_1^{(t)},\cdots,m_l^{(t)},\cdots,m_k^{(t)})$$，计算每个样本到类中心的距离，将每个样本指派到与其最近的中心的类中，构成聚类结果$$C^{(t)}$$。
&emsp;&emsp;(3)计算新的聚类中心。对聚类结果$$C^{(t)}$$，计算当前各个勒种的样本的均值，作为新的类中心$$m^{(t+1)}=(m_1^{(t+1)},\cdots,m_l^{(t+1)},\cdots,m_k^{(t+1)})$$。
&emsp;&emsp;(4)如果迭代收敛或符合停止条件，输出$$C*=C^{(t)}$$
&emsp;&emsp;否则令t=t+1，返回步骤二
&emsp;&emsp;给定含5个样本的集合
k均值聚类算法的复杂度是O(mnk)，m是样本维数，n为样本个数，k为类别个数。
</font>

**例子**
<font face="黑体" size=4> &emsp;&emsp;
&emsp;&emsp;给定含5个样本的集合
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;![](/uploads/zhishifenxiang/images/m_b16811ff432f52719a421642ce4210c9_r.png)
&emsp;&emsp;k均值聚类到两个类中</font>
**计算：**
<font face="黑体" size=4> &emsp;&emsp;
&emsp;&emsp;（1）选择两个样本点作为类的中心。假设$$m_1^{(0)}=x_1=(0,2)^T,m_2^{(0)}=x_2=(0,0)^T$$。
&emsp;&emsp;（2）以$$m_1^{(0)}$$,$$m_2^{(0)}$$为类$$G_1^{(0)}$$,$$G_2^{(0)}$$的中心，计算$$x_3=(1,0)^T$$,$$x_4=(5,0)^T$$,$$x_5=(5,2)^T$$与$$m_1^{(0)}=(0,2)^T$$,$$m_2^{(0)}=(0,0)^T$$的欧氏距离平方。
&emsp;&emsp;对$$x_3=(1,0)^T,d(x_3,m_1^{(0)})=5,d(x_3,m_2^{(0)})=1$$，将$$x_3$$分到类$$G_2^{(0)}$$。
&emsp;&emsp;对$$x_4=(5,0)^T,d(x_4,m_1^{(0)})=29,d(x_4,m_2^{(0)})=25$$，将$$x_4$$分到类$$G_2^{(0)}$$。
&emsp;&emsp;对$$x_5=(5,2)^T,d(x_5,m_1^{(0)})=25,d(x_5,m_2^{(0)})=29$$，将$$x_5$$分到类$$G_1^{(0)}$$。
&emsp;&emsp;（3）得到新的类$$G_1^{(1)}=\lbrace x_1 x_5 \rbrace$$,$$G_2^{(1)}=\lbrace x_2,x_3 x_4 \rbrace$$计算类的中心$$m_1^{(1)},m_2^{(1)}$$:
$$m_1^{(1)}=(2.5,2.0)^T,m_2^{(1)}=(2,0)^T$$
&emsp;&emsp;（4）重复步骤（2）和（3）
&emsp;&emsp;将$$x_1$$分到$$G_1^{(1)}$$,$$x_2$$分到$$G_2^{(1)}$$，$$x_3$$分到$$G_2^{(1)}$$，$$x_5$$分到$$G_1^{(1)}$$。
&emsp;&emsp;得到新的类$$G_1^{(2)}=\lbrace x_1 x_5 \rbrace$$,$$G_2^{(2)}=\lbrace x_2,x_3 x_4 \rbrace$$
&emsp;&emsp;由于得到的新的类没有改变，聚类停止。得到聚类结果：
$$G_1^\*=\lbrace x_1 x_5 \rbrace$$，$$G_2^{*}=\lbrace x_2,x_3 x_4 \rbrace$$
</font>
#### 3.3KNN分类（K近邻）与K-Means聚类（K均值）的区别

|KNN|k-Means|
|----|----|
|目的是为了确定一个点的分类|目的是为了将一系列点集分成k类|
|KNN是分类算法|K-Means是聚类算法|
|监督学习，分类目标事先已知|非监督学习，将相似数据归到一起从而得到分类，没有外部分类|
|训练数据集有label，已经是完全正确的数据|训练数据集无label，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序|
|没有明显的前期训练过程，属于memory-based learning|有明显的前期训练过程|
|K的含义：“k”是用来计算的相邻数据数。来了一个样本x，要给它分类，即求出它的y，就从数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c|K的含义：“k”是类的数目。K是人工固定好的数字，假设数据集合可以分为K个簇，由于是依靠人工定好，需要一点先验知识|
|K值确定后每次结果固定|K值确定后每次结果可能不同，从 n个数据对象任意选择 k 个对象作为初始聚类中心，随机性对结果影响较大|
|时间复杂度：O（n）|时间复杂度：O(n*k*t)，t为迭代次数|


2.层次聚类
3.k均值聚类
k均值聚类与k近邻的一点区别
层次聚类的应用
k均值的应用
rcnn的区域划分selection search 使用聚类思想
图像分割