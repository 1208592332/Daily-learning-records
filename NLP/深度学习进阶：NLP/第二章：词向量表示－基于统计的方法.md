<!--深度学习进阶：NLP-->

书籍：

１．斋藤康毅的深度学习神作《深度学习入门：基于Python的理论与实现》

２．《深度学习进阶：自然语言处理》

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/12448303/1610547598141-95a4d354-ec99-446e-a30c-5a1c895b476a.jpeg?x-oss-process=image%2Fresize%2Cw_527)

正文开始

## 目录

- 什么是自然语言

- 同义词典

- 基于统计的方法

- 基于统计的方法改进

  

## 　同义词典

最著名的同义词词典当属WordNet 。WordNet等同义词词典中对大量单词人工的定义了同义词和层级结构关系等。

### 同义词典存在的问题

- 难以顺应时代的变化。语言是活的，新词会不断出现。

- 人力成本高，WordNet收录了超过20W个单词。

- 无法表示单词的微妙差异。即使是含义相近的单词，也有细微的差别。比如，vintage 和retro（类似复古的意思）虽然表示相同的含义，但是用法不同，而这种细微的差别在同义词词典中是无法表示出来的(让人来解释是相当困难的)。

## 基于统计的方法（基于统计）

目标：从海量文本数据中自动提取单词含义，减少人为干扰。

- 语料库（corpus）：就是我们输入模型的大量文本，比如句子、文章等等。

这里将用一句话作为语料库来阐述接下来的所有概念。

```python
text = 'you say goodbye and I say hello.'
```

### 语料库的处理

1、**进行句子的分词，并标记每个单词的ID。**

```powershell
>>> text = text.lower()  # 将所有单词转化为小写
>>> text = text.replace('.', ' .') # 使句号其和前一个单词分开
>>> text
'you say goodbye and i say hello .'
>>> words = text.split(' ') # 切分句子
>>> words
['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.'] # 由八个词组成的数组
```

首先，使用 `lower()` 方法将所有单词转化为小写，这样可以将句子开头的单词也作为常规单词处理。然后，将 `空格` 作为分隔符，通过 `split(' ')` 切分句子。考虑到句子结尾处的句号(.),我们先在句号前插人一个空格(即用 ` ' .'` 替换 `'.'` ),再进行分词。

2、**我们进一步给单词标上 ID,以便使用单词 ID 列表**，方便为后续对每个单词进行操作。

将单词列表转化为单词 ID 列表,然后再将其转化为 NumPy 数组。

```powershell
word_to_id = {}  # 将单词转化为单词 ID
id_to_word = {}  # 将单词 ID 转化为单词(键是单词ID,值是单词) 
for word in words: 
  		if word not in word_to_id:   
  		# 如果单词不在 word_to_id 中,则分别向 word_to_id 和id_to_word 添加新 ID 和单词
    			new_id = len(word_to_id) 
    			word_to_id[word] = new_id 
    			id_to_word[new_id] = word
corpus = np.array([word_to_id[w] for w in words])
```

如果单词不在 word_to_id 中,则分别向 word_to_id 和id_to_word 添加新 ID 和单词

**如下为创建好了单词 ID 和单词的对应表**

```powershell
>>> id_to_word 
{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6:'.'} 

>>> word_to_id 
{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}
```

最后将单词列表转换为单词ＩＤ列表，然后再转化为Ｎｕｍｐｙ数组

```powershell
>>> corpus 
array([0, 1, 2, 3, 4, 1, 5, 6])
```

将第一步和第二步封装为一个 `preprocess()` 函数，使用这个函数,可以按如下方式对语料库进行**预处理**。( 代码在 `common/util.py` ) 

输入要处理的语料库 `text` ，输出 `corpus, word_to_id, id_to_word` 

```powershell
>>> text = 'You say goodbye and I say hello.' 
>>> corpus, word_to_id, id_to_word = preprocess(text)
```

`corpus` 是单词ID 列表, `word_to_id` 是单词到单词 ID 的字典, `id_to_word` 是 ID 到单词的字典。

语料库的预处理已完成。这里准备的 `corpus、word_to_id ` 和 ` id_to_word` 这 3 个变量名在本书接下来的很多地方都会用到。(因为文本预处理完了就要准备开始用了呀！

接下来的目标就是 `用语料库提取单词含义` ，这里先使用基于计数的方法，也就是 `基于统计` 的方法，能够得到词向量！（也就是将单词表示为向量）

### 分布式假说（distributional hypothesis)

- 分布式假说（distributional hypothesis)：某个单词的含义由它周围的单词形成。

单词本身没有含义,单词含义由它所在的上下文(语境)形成。

比如 `“I drink beer.”` `“We drink wine.”` , `drink` 的附近常有 `饮料` 出现。

另外,从 `“I guzzle beer.”`  `“We guzzle wine.”`可知,`guzzle 和 drink` 所在的语境相似。进而我们可以推测出`guzzle 和 drink` 是近义词(`guzzle` 是“大口喝”的意思) 。

基于这一假说，我们就可以通过单词的上下文来表示该单词。如图，左侧和右侧的 2 个单词就是上下文。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/12448303/1610547598156-7b0bd202-86b3-4896-aaa6-e0f7ee01947d.jpeg?x-oss-process=image%2Fresize%2Cw_527)

这里的“２”，即窗口的大小，关心多少个单词的上下文。关心的上下文单词数越多，单词的含义就越准确，但是所需要的存储量就越大，看你自己的取舍咯！这里简单起见，窗口大小为1。

如何基于分布式假设使用向量表示单词,最直截了当的实现方法是对周围单词的数量进行计数。

### 共现矩阵（co-occurrence matrix)（第一个表示词向量的简单方法）

- 共现矩阵（co-occurrence matrix)：用上下文共同出现的单词次数作为该单词的向量。即若两个单词挨着出现一次，次数加一。

上面已经处理好语料库了，接下来构建共现矩阵，也就是计算每个单词的上下文所包含的单词的频数。在这个例子中,我们将窗口大小设为 `1`,从单词 ID 为 `0` 的` you` 开始。

单词 you 的上下文仅有` say` 这个单词，且出现了一次，如下图所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/12448303/1610547598127-41cd88dc-d95a-4aa7-bd7e-fa4409f103d8.jpeg?x-oss-process=image%2Fresize%2Cw_527)

所以单词`you`可表示为：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/12448303/1610547598156-ff40fa99-9d1d-4a1b-817f-c4ee2aa89a00.jpeg?x-oss-process=image%2Fresize%2Cw_527)

即用向量` [0, 1, 0, 0, 0, 0, 0]` 表示单词` you`，这就是最简单的用向量表示词的方法啦。其他单词也是重复如此操作。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/12448303/1610547598157-b1e3e248-dc2b-4576-b06e-b9015f78d96c.jpeg?x-oss-process=image%2Fresize%2Cw_527)

所有词都表示完成之后，于是得到了共现矩阵：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/12448303/1610547598181-86fd732a-4be0-4e8e-a601-7a9916082576.jpeg?x-oss-process=image%2Fresize%2Cw_527)

接下来,我们用`代码`来实际创建一下上面的共现矩阵。将图 2-7 的结果手动输入。

```powershell
C = np.array([
  [0, 1, 0, 0, 0, 0, 0],
  [1, 0, 1, 0, 1, 1, 0], 
  [0, 1, 0, 1, 0, 0, 0], 
  [0, 0, 1, 0, 1, 0, 0], 
  [0, 1, 0, 1, 0, 0, 0], 
  [0, 1, 0, 0, 0, 0, 1], 
  [0, 0, 0, 0, 0, 1, 0], ], dtype=np.int32)
```

这就是共现矩阵。

插入一个题外话，这里的`dtype=np.int32`指的是位精度，

从内存的角度来看，32 位只有 64 位的一半，所以通常首选 32 位。

在神经网络的计算中，数据传输的总线带宽有时会成为瓶颈。在这种情况下，毫无疑问数据类型也是越小越好。

使用这个共现矩阵,可以获得各个单词的向量,如下所示。

```powershell
print(C[0]) # 单词ID为0的向量，也就是you
# [0 1 0 0 0 0 0] 
print(C[4]) # 单词ID为4的向量
# [0 1 0 1 0 0 0] 
print(C[word_to_id['goodbye']]) # goodbye的向量
# [0 1 0 1 0 0 0]
```

我们通过共现矩阵成功地用向量表示了单词！但手动输入共现矩阵太麻烦,这一操作显然可以`自动化`。下面,我们来实现一个能直接从语料库生成共现矩阵的函数。 

通过函数`create_co_matrix()`能直接从语料库生成共现矩阵。（代码实现在`common/util.py`）

其中参数 `corpus `是单词 ID 列表,参数` vocab_ size `是词汇个数,`window_size` 是窗口大小。

```python
def create_co_matrix(corpus, vocab_size, window_size=1):   
  	corpus_size = len(corpus) 
  	co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)  # 构建全0二维数组，用于存储共现矩阵
  
  	for idx, word_id in enumerate(corpus): 
    		for i in range(1, window_size + 1): 
      			left_idx = idx - i 
      			right_idx = idx + i 
      
      			if left_idx >= 0: 
       					 left_word_id = corpus[left_idx] 
        				co_matrix[word_id, left_word_id] += 1 
        
     			 if right_idx < corpus_size:
       					right_word_id = corpus[right_idx] 
      					 co_matrix[word_id, right_word_id] += 1 
    
   			 return co_matrix
```

首先,用全0的二维数组对` co_matrix `进行初始化。`enumerate(corpus) `将`corpus`单词ID列表组合成索引序列。然后,针对语料库中的每一个单词,计算它的窗口中包含的单词。同时,检查窗口内的单词是否超出了语料库的左端和右端。

这样一来,无论语料库多大,都可以自动生成共现矩阵。之后,我们都将使用这个函数`create_co_matrix()`来生成共现矩阵。 

到这里我们终于第一次成功的用向量表示单词啦！将正式迈入文本词向量表示的道路！





[参考链接　云不见：深度学习进阶](https://www.yuque.com/yunbujian/nlp/lxas00#Ayfgh)